{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANWENDUNGEN DER KI CHALLENGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vorgehensweise:\n",
    "1. TFIDF verwenden um relevante Wörter aus dem Text zu extrahieren\n",
    "2. Häufige, aber irrelevante/neutrale Wörter entfernen\n",
    "3. Daten klassifizieren und zu einem mehrdimensionalen Feld aus klassifizierten Daten zusammenfassen\n",
    "4. Multinomialer Naiver Bayes trainieren\n",
    "5. Testdaten in trainiertes Modell geben und Resultat auswerten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Florian\\Anaconda3_2018\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import scipy.sparse as sp\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExtractedData(data_path):\n",
    "    # Get file and read all words in it\n",
    "    file = open(data_path, 'r')\n",
    "    words_list = file.read().split()\n",
    "    \n",
    "    # Count words and create dictionary (set of words)\n",
    "    #dictionary = Counter(words_list)\n",
    "    # Clean words set, remove neutral and non-relevant words\n",
    "    #del dictionary['the']\n",
    "    #del dictionary['and']\n",
    "    #del dictionary['a']\n",
    "    #del dictionary['that']\n",
    "    #del dictionary['this']\n",
    "    #TODO: use TFIDF to find out the files which are most_common releative to number of words / Currently: Bag of words\n",
    "    #dictionary = dictionary.most_common(700) \n",
    "    #only revert \n",
    "    #words_list = []\n",
    "    #for i in dictionary:\n",
    "    #    words_list.append(i[0])\n",
    "    #print(words_list)\n",
    "    return words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labelData(data, label):\n",
    "    data_labels = []\n",
    "    for i in data:\n",
    "        data_labels.append(label)\n",
    "    return data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "data_path_neg = 'C:\\\\Users\\\\Florian\\\\Desktop\\\\Programmierprojekt Python\\\\AppliedKI\\\\test.txt'\n",
    "data_path_pos = 'C:\\\\Users\\\\Florian\\\\Desktop\\\\Programmierprojekt Python\\\\AppliedKI\\\\train-pos.txt'\n",
    "data_path_test = 'C:\\\\Users\\\\Florian\\\\Desktop\\\\Programmierprojekt Python\\\\AppliedKI\\\\train-pos.txt'\n",
    "\n",
    "train_data_neg = getExtractedData(data_path_neg)\n",
    "train_data_pos = getExtractedData(data_path_pos)\n",
    "test_data = getExtractedData(data_path_test)\n",
    "\n",
    "# Label the data according to occurance\n",
    "neg_labels = labelData(train_data_neg, 'negative')\n",
    "pos_labels = labelData(train_data_pos, 'positive')\n",
    "\n",
    "# Use count vectorizer to convert collection of text to matrix of token counts\n",
    "vectorizer = CountVectorizer(analyzer='word')\n",
    "features_neg = list(vectorizer.fit_transform(train_data_neg))\n",
    "features_pos = list(vectorizer.fit_transform(train_data_pos))\n",
    "\n",
    "print(features_neg)\n",
    "\n",
    "\n",
    "# Convert all data to arrays\n",
    "#X_train, Y_train, X_test, Y_test = train_test_split(sp.vstack((features_neg, features_pos)), neg_labels+pos_labels)\n",
    "\n",
    "\n",
    "# Train the model \n",
    "log_model = LogisticRegression().fit(X=X_train, Y=Y_train)\n",
    "\n",
    "# Predict the test data\n",
    "y_pred = log_model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
